{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KU69YG_noY2Q"
      },
      "source": [
        "# Evolutionary Strategies\n",
        "\n",
        "<a rel=\"license\" href=\"http://creativecommons.org/licenses/by-sa/4.0/\"><img alt=\"Creative Commons License\" align=\"left\" src=\"https://i.creativecommons.org/l/by-sa/4.0/80x15.png\" /></a>&nbsp;| Dennis G. Wilson | <a href=\"https://rl-vs.github.io/rlvs2021/\">https://rl-vs.github.io/rlvs2021/</a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0i0T9La8oY2U"
      },
      "source": [
        "## Overview\n",
        "\n",
        "1. [$(1 + \\lambda)$ ES](#oneplus)\n",
        "2. [Approximating the gradient](#gradient)\n",
        "3. [ES for Neuroevolution](#neuroevolution)\n",
        "4. [Canonical ES](#canonical)\n",
        "5. [CMA-ES](#cmaes)\n",
        "6. [CMA-ES for Neuroevolution](#cmaes_neuro)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JqIKlVghoY2V"
      },
      "outputs": [],
      "source": [
        "#!apt-get install -y xvfb python-opengl > /dev/null 2>&1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "P6vyvNo_oY2X"
      },
      "outputs": [],
      "source": [
        "#!pip install cma pyvirtualdisplay gym[box2d]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wz2y25dUoY2X"
      },
      "source": [
        "# <a name=\"oneplus\">1.</a> The $(1 + \\lambda)$ ES"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ey1VNuTuoY2Y"
      },
      "source": [
        "The simplest Evolutionary Strategy is the $(1+1)$. In this algorithm, the best individual generated is replaced by a new individual when its **fitness** is superior to the current best.\n",
        "\n",
        "    Initialize x randomly in ‚Ñù\n",
        "    while not terminate\n",
        "        x' = x + ùëÅ(0, 1)\n",
        "        if f(x‚Ä≤) < f(x)\n",
        "            x = x'\n",
        "    return x"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DjWLwkFuoY2Z"
      },
      "source": [
        "In this algorithm, at each step, we randomly sample from a Normal distribution to get one new point near the current point. However, to better inform the movement from this initial point, we could sample multiple points instead of a single one. We'll first look at examples of continuous functions, as their search space is easier to visualize."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x5T_gPGdoY2a"
      },
      "outputs": [],
      "source": [
        "def himmelblau(x, y):\n",
        "    return (x**2 + y - 11)**2 + (x + y**2 - 7)**2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DokGPMI3oY2b"
      },
      "outputs": [],
      "source": [
        "def rosenbrock(x, y, a=1, B=100):\n",
        "      return (a-x)**2 + B*((y-x**2))**2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nehdSXFGoY2c"
      },
      "outputs": [],
      "source": [
        "%matplotlib inline\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib import ticker, cm\n",
        "import matplotlib.colors as colors\n",
        "import numpy as np\n",
        "import math"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dlJPBs_ooY2c"
      },
      "outputs": [],
      "source": [
        "X = np.arange(-5, 5, 0.1)\n",
        "Y = np.arange(-5, 5, 0.1)\n",
        "X, Y = np.meshgrid(X, Y)\n",
        "Z = himmelblau(X, Y)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "04j121IIoY2d"
      },
      "outputs": [],
      "source": [
        "fig = plt.figure(figsize=(8, 6))\n",
        "cs = plt.contour(X, Y, Z, levels=100, cmap='Spectral',\n",
        "                 norm=colors.Normalize(vmin=Z.min(), vmax=Z.max()), alpha=0.4)\n",
        "fig.colorbar(cs)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xMQ73pQkoY2d"
      },
      "source": [
        "We'll start with a random point $x$. We'll then use a Normal distribution to draw 20 points around $x$."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q_hvlmSKoY2e"
      },
      "outputs": [],
      "source": [
        "x = np.random.rand(2) * 4 - 2\n",
        "print(\"position: \", x, \"fitness :\", himmelblau(x[0], x[1]))\n",
        "x_t = np.array([x + np.random.normal(size=(2,)) for i in range(20)])\n",
        "fig = plt.figure(figsize=(8, 6))\n",
        "cs = plt.contour(X, Y, Z, levels=100, cmap='Spectral', norm=colors.Normalize(vmin=Z.min(), vmax=Z.max()), alpha=0.4)\n",
        "plt.scatter(x_t[:, 0], x_t[:, 1], c='b')\n",
        "plt.scatter(x[0], x[1], c='r')\n",
        "fig.colorbar(cs);"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-BOyYV2toY2f"
      },
      "source": [
        "Having this population of points will enable us to explore the space around $x$ before moving on to the next step, which can better inform movement in the search space. However, these points might overlap or cover spaces that have already been explored. Generating too many points could slow down search, requiring more evaluations of the objective function. In many applications, that is costly."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BOYzanP9oY2h"
      },
      "source": [
        "Performing random optimization but sampling more than 1 point leads to the $(1+\\lambda)$ Evolutionary Strategy. \n",
        "\n",
        "    Initialize x randomly in ‚Ñù\n",
        "    while not terminate\n",
        "        x_p = x\n",
        "        for i in [1,Œª]\n",
        "            x_i = x_p + ùëÅ(0, 1)\n",
        "            if f(x_i) < f(x)\n",
        "                x = x_i\n",
        "        x_p = x\n",
        "    return x_p"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YZGyG1xvoY2h"
      },
      "source": [
        "In the scope of evolutionary algorithms, we'll refer to $x$ as a parent and all $x'$ points as offspring. Each point can also be referred to as an individual in a population, and $f(x)$ is called the fitness of the individual. Each iteration of the algorithm is called a generation."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YzK8mGkPoY2i"
      },
      "source": [
        "<div class=\"alert alert-info\">\n",
        "\n",
        "The $(\\mu/\\rho,\\lambda)$ or $(\\mu/\\rho+\\lambda)$ notation signifies the configuration of parents and offspring. In this notation, $\\mu$ is the number of parents, $\\rho$ is the number of parents involved in creating the offspring, and $\\lambda$ is the number of offspring. $(\\mu/\\rho+\\lambda)$ means the parents can be included in the next population, whereas $(\\mu/\\rho,\\lambda)$ means the parents are not included. $(1+\\lambda)$ therefore means that 1 parent is involved in creating a population of $\\lambda$ offspring and can be included in the next generation (ie, if $f(x) < f(x') \\forall x'$, $x$ does not change).\n",
        "    \n",
        "</div>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z-or31VeoY2i"
      },
      "outputs": [],
      "source": [
        "def oneplus_lambda(x, fitness, gens=100, lam=20):\n",
        "    x_best = x\n",
        "    f_best = fitness(x)\n",
        "    fits = np.zeros(gens)\n",
        "    for g in range(gens):\n",
        "        N = np.random.normal(size=(lam, len(x)))\n",
        "        for i in range(lam):\n",
        "            ind = x + N[i, :]\n",
        "            f = fitness(ind)\n",
        "            if f < f_best:\n",
        "                f_best = f\n",
        "                x_best = ind\n",
        "        x = x_best\n",
        "        fits[g] = f_best\n",
        "    return fits"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5zfHVQOaoY2j"
      },
      "source": [
        "Let's run the $(1+\\lambda)$ ES on the himmelblau function and record the evolution over multiple trials."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wQYha-T9oY2j"
      },
      "outputs": [],
      "source": [
        "f = lambda x : himmelblau(x[0], x[1])\n",
        "runs = []\n",
        "for i in range(5):\n",
        "    x = np.random.rand(2)*10-5\n",
        "    runs += [oneplus_lambda(x, f)]\n",
        "runs = np.array(runs)\n",
        "runs_mean = np.mean(runs, 0)\n",
        "runs_std = np.std(runs, 0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_Jr5jWEioY2k"
      },
      "outputs": [],
      "source": [
        "fig = plt.figure(figsize=(8, 6))\n",
        "plt.fill_between(range(len(runs[0])), runs_mean+0.5*runs_std, runs_mean-0.5*runs_std, alpha=0.5)\n",
        "plt.plot(range(len(runs[0])), runs_mean)\n",
        "plt.yscale('log')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MWsIyI4doY2k"
      },
      "source": [
        "<div class=\"alert alert-success\">\n",
        "    <h3>Exercise 1</h3>\n",
        "    Study the impact of the $\\lambda$ parameter by modifying it and re-running the optimization. What is the best $\\lambda$ value for the Himmelblau problem?\n",
        "</div>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Nas_Dbi_oY2l"
      },
      "source": [
        "# <a name=\"gradient\">2.</a> Approximating the gradient"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2oyQsmj6oY2n"
      },
      "source": [
        "In the $(1+\\lambda)$ ES, we move from the best point in a population to the best point in the next randomly sampled population. However, with the population information, we can do better than that; we can move in the direction of the gradient approximated by the fitness values of the population. This can also better use the full population information, combining all fitness information instead of selecting the best fitness value."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ILH315_OoY2n"
      },
      "outputs": [],
      "source": [
        "x = np.random.rand(2) * 4 - 2\n",
        "x_t = np.array([x + np.random.normal(size=(2,)) for i in range(10)])\n",
        "fits = himmelblau(x_t[:, 0], x_t[:, 1])\n",
        "\n",
        "fig = plt.figure(figsize=(8, 6))\n",
        "norm = colors.Normalize(vmin=Z.min(), vmax=Z.max())\n",
        "cs = plt.contour(X, Y, Z, levels=100, cmap='Spectral', alpha=0.4, norm=norm)\n",
        "plt.quiver(x_t[:, 0], x_t[:, 1], x_t[:, 0]-x[0], x_t[:, 1]-x[1],\n",
        "           fits, scale=fits/100, scale_units='xy', cmap='Spectral', norm=norm)\n",
        "plt.scatter(x[0], x[1], c='k')\n",
        "fig.colorbar(cs)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4bw2Hy51oY2o"
      },
      "source": [
        "First we normalize all fitness values by the average $\\mu(f(x))$ and standard deviation $\\sigma(f(x))$ of the population. This is to make the movement based on the relative fitness in the population as opposed to the absolute fitness in the search space.\n",
        "\n",
        "$A = \\frac{f(x) - \\mu(f(x))}{\\sigma(f(x))}$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NQ3SqDcCoY2o"
      },
      "source": [
        "We'll then define a vector at each point, $x_i - x$. Note that since $x_i = x + N(0, 1)$, we could store just this original random sampled point from $N(0, 1)$ as our vector.\n",
        "\n",
        "$N_i = x_i - x$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QS8cEKcSoY2p"
      },
      "source": [
        "We multiply $A$ by all individual vectors which scales the vector magnitude by the relative fitness. Finally, we sum all vectors and divide by $\\lambda$ to get the weighted average.\n",
        "\n",
        "$\\frac{1}{\\lambda}\\sum_i A_i N_i$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hd1qgtdboY2p"
      },
      "source": [
        "This is equivalent to the dot product and is used as our gradient approximation. Since we're minimizing, we'll use the negative value to do gradient descent.\n",
        "\n",
        "$\\nabla f \\approx -\\frac{A \\cdot N}{\\lambda}$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n8F0dRw8oY2q"
      },
      "outputs": [],
      "source": [
        "A = (fits - np.mean(fits)) / np.std(fits)\n",
        "N = x_t - x\n",
        "G = -np.dot(A, N) / 10"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yQreEZkroY2u"
      },
      "outputs": [],
      "source": [
        "fig = plt.figure(figsize=(8, 6))\n",
        "norm = colors.Normalize(vmin=Z.min(), vmax=Z.max())\n",
        "cs = plt.contour(X, Y, Z, levels=100, cmap='Spectral', alpha=0.4, norm=norm)\n",
        "plt.quiver(x_t[:, 0], x_t[:, 1], x_t[:, 0]-x[0], x_t[:, 1]-x[1],\n",
        "           fits, scale=fits/100, scale_units='xy', cmap='Spectral', norm=norm)\n",
        "plt.quiver(x[0], x[1], G[0], G[1])\n",
        "plt.scatter(x[0], x[1], c='k')\n",
        "fig.colorbar(cs)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bx5v4W6foY2u"
      },
      "source": [
        "We have our approximate gradient direction, but how far should we move in it? We'll define a learning rate variable $\\alpha$ as the fixed magnitude of movement for now, so our update of $x$ is then\n",
        "\n",
        "$x = x - \\alpha\\frac{A \\cdot N}{\\lambda}$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x4D7WbuloY2u"
      },
      "source": [
        "Let's put this all together as an Evolutionary Strategy. It is a $(\\mu, \\lambda)$ ES as multiple individuals $\\mu$ inform the next generation; however, these individuals are not kept for the next generation.\n",
        "\n",
        "    Initialize x randomly in ‚Ñù\n",
        "    while not terminate\n",
        "        for i in [1,Œª]\n",
        "            N_i = ùëÅ(0, 1)\n",
        "            F_i = f(x + N_i)\n",
        "        A = (F‚àíùúá(F))/ùúé(F)\n",
        "        x = x - ùõº(A‚ãÖN)/ùúÜ"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jkUXgYPUoY2v"
      },
      "outputs": [],
      "source": [
        "def mu_lambda(x, fitness, gens=200, lam=10, alpha=0.2, verbose=False):\n",
        "    x_best = x\n",
        "    f_best = fitness(x)\n",
        "    fits = np.zeros(gens)\n",
        "    for g in range(gens):\n",
        "        N = np.random.normal(size=(lam, len(x)))\n",
        "        F = np.zeros(lam)\n",
        "        for i in range(lam):\n",
        "            ind = x + N[i, :]\n",
        "            F[i] = fitness(ind)\n",
        "            if F[i] < f_best:\n",
        "                f_best = F[i]\n",
        "                x_best = ind\n",
        "                if verbose:\n",
        "                    print(g, \" \", f_best)\n",
        "        fits[g] = f_best\n",
        "        mu_f = np.mean(F)\n",
        "        std_f = np.std(F)\n",
        "        A = F\n",
        "        if std_f != 0:\n",
        "            A = (F - mu_f) / std_f\n",
        "        x = x - alpha * np.dot(A, N) / lam\n",
        "    return fits, x_best"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IIDghTDzoY2w"
      },
      "outputs": [],
      "source": [
        "runs = []\n",
        "for i in range(5):\n",
        "    x = np.random.randn(2)\n",
        "    fits, _ = mu_lambda(x, f)\n",
        "    runs += [fits]\n",
        "runs = np.array(runs)\n",
        "runs_mean = np.mean(runs, 0)\n",
        "runs_std = np.std(runs, 0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CViLLIleoY2w"
      },
      "outputs": [],
      "source": [
        "fig = plt.figure(figsize=(8, 6))\n",
        "plt.fill_between(range(len(runs[0])), runs_mean+0.5*runs_std, runs_mean-0.5*runs_std, alpha=0.5)\n",
        "plt.plot(range(len(runs[0])), runs_mean)\n",
        "plt.yscale('log')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5bB4OV19oY2x"
      },
      "source": [
        "<div class=\"alert alert-success\">\n",
        "    <h3>Exercise 2</h3>\n",
        "    \n",
        "Study the impact of the $\\alpha$ parameter by modifying it and re-running the optimization. Compare the best parameters found with the results from the $(1+\\lambda)$ ES.\n",
        "</div>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6P0dvVyUoY2x"
      },
      "source": [
        "# <a name=\"neuroevolution\">3.</a> ES for Neuroevolution"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FmsmqigUoY2y"
      },
      "source": [
        "Evolutionary strategies are intended for continuous optimization and can easily be applied to the optimization of neural network parameters, or *neuroevolution*. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jIFFnJpioY2y"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.multiprocessing as mp\n",
        "import gym"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oAvvQZyboY2y"
      },
      "outputs": [],
      "source": [
        "class NeuralNetwork(nn.Module):\n",
        "\n",
        "    def __init__(self, input_shape, n_actions):\n",
        "        super(NeuralNetwork, self).__init__()\n",
        "        self.l1 = nn.Linear(input_shape, 32)\n",
        "        self.l2 = nn.Linear(32, 32)\n",
        "        self.lout = nn.Linear(32, n_actions)\n",
        "        \n",
        "    def forward(self, x):\n",
        "        x = F.relu(self.l1(x.float()))\n",
        "        x = F.relu(self.l2(x))\n",
        "        return self.lout(x)\n",
        "    \n",
        "    def get_params(self):\n",
        "        p = np.empty((0,))\n",
        "        for n in self.parameters():\n",
        "            p = np.append(p, n.flatten().cpu().detach().numpy())\n",
        "        return p\n",
        "    \n",
        "    def set_params(self, x):\n",
        "        start = 0\n",
        "        for p in self.parameters():\n",
        "            e = start + np.prod(p.shape)\n",
        "            p.data = torch.FloatTensor(x[start:e]).reshape(p.shape)\n",
        "            start = e"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DPAl1UjOoY2z"
      },
      "source": [
        "We'll add some visualization functionality to have the environment render directly in the notebook. To run this notebook in Google colab, uncomment and run the following lines."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X7CHWYW2oY2z"
      },
      "outputs": [],
      "source": [
        "from pyvirtualdisplay import Display\n",
        "from IPython import display\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "pydisplay = Display(visible=0, size=(1400, 900))\n",
        "pydisplay.start()\n",
        "plt.ion();"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SKtW8y_koY2z"
      },
      "source": [
        "Following the framework of evolutionary policy search, we will optimize a neural network representing a policy and maximize the total reward over a single episode using this policy."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SvSOV3mdoY2z"
      },
      "outputs": [],
      "source": [
        "def evaluate(ann, env, visul=True):\n",
        "    env.seed(0) # deterministic for demonstration\n",
        "    obs = env.reset()\n",
        "    if visul:\n",
        "        img = plt.imshow(env.render(mode='rgb_array'))\n",
        "    total_reward = 0\n",
        "    while True:\n",
        "        # Output of the neural net\n",
        "        net_output = ann(torch.tensor(obs))\n",
        "        # the action is the value clipped returned by the nn\n",
        "        action = net_output.data.cpu().numpy().argmax()\n",
        "        obs, reward, done, _ = env.step(action)\n",
        "        total_reward += reward\n",
        "        if visul:\n",
        "            img.set_data(env.render(mode='rgb_array')) \n",
        "            plt.axis('off')\n",
        "            display.display(plt.gcf())\n",
        "            display.clear_output(wait=True)\n",
        "        if done:\n",
        "            break\n",
        "    return total_reward"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XjEIDaH6oY20"
      },
      "source": [
        "We've configured this for discrete action spaces. We can see a random neural network on different environments like `CartPole-v0`, `MountainCar-v0`, and `LunarLander-v2`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F3yFJNF3oY20"
      },
      "outputs": [],
      "source": [
        "env = gym.make('LunarLander-v2')\n",
        "ann = NeuralNetwork(env.observation_space.shape[0], env.action_space.n)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bijUWQJUoY20"
      },
      "outputs": [],
      "source": [
        "evaluate(ann, env, visul=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NVJgn9_GoY21"
      },
      "source": [
        "In order to evolve the parameters of this neural network, we will modify the parameters of the network using `set_params` with the genes of the new individual. In the evolutionary literature, this is referred to as a *direct encoding* as the neural network parameters are directly encoded in the genome."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3njDMfzOoY21"
      },
      "outputs": [],
      "source": [
        "def fitness(x, ann, env, visul=False):\n",
        "    ann.set_params(x)\n",
        "    return -evaluate(ann, env, visul=visul)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0Cj4gIKyoY21"
      },
      "outputs": [],
      "source": [
        "p = ann.get_params()\n",
        "np.shape(p)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z23_bhDLoY22"
      },
      "source": [
        "We can first observe a random individual $x$."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NN4bS9F6oY22"
      },
      "outputs": [],
      "source": [
        "x = np.random.rand(len(p))\n",
        "-fitness(x, ann, env, visul=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ms9ofLwaoY22"
      },
      "outputs": [],
      "source": [
        "np.random.seed(654)\n",
        "env = gym.make('LunarLander-v2')\n",
        "ann = NeuralNetwork(env.observation_space.shape[0], env.action_space.n)\n",
        "x = np.random.randn(len(ann.get_params()))\n",
        "f = lambda x : fitness(x, ann, env)\n",
        "fits, x = mu_lambda(x, f, gens=30, lam=10, alpha=0.1, verbose=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tf3cBpxuoY23"
      },
      "outputs": [],
      "source": [
        "fits"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "czLxlAinoY23"
      },
      "outputs": [],
      "source": [
        "plt.plot(fits);"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iuvnQcd7oY23"
      },
      "outputs": [],
      "source": [
        "-fitness(x, ann, env, visul=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ls8dCS1zoY24"
      },
      "source": [
        "<div class=\"alert alert-success\">\n",
        "    <h3>Exercise 3</h3>\n",
        "    \n",
        "The size of the neural network changes the search space of possible policy functions. Try modifying the neural network definition by changing the size of the intermediary layers or adding new layers. Does the evolution change?\n",
        "</div>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-w5-BX_qoY24"
      },
      "source": [
        "# <a name=\"canonical\">4.</a> Canonical ES"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oJ2z1_-IoY24"
      },
      "source": [
        "We can make two observations about our current $(\\mu, \\lambda)$ Evolutionary Strategy:\n",
        "\n",
        "1. Not all $\\lambda$ points may be useful for the centroid update\n",
        "2. Exact gradient information may be less useful than relative *rank* of individual fitness"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qg_pTRKdoY25"
      },
      "source": [
        "Let's illustrate this point in the Himmelblau function. Instead of using all $\\lambda$ points, we'll use a smaller value of $\\mu$, for example $\\frac{1}{2}\\lambda$. We'll also create a list of weights which exponentially decreases from the first to last weight."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qFIzU6AooY25"
      },
      "outputs": [],
      "source": [
        "mu = 5\n",
        "w = np.log(mu + 1/2) - np.log(np.arange(1, mu+1))\n",
        "w /= np.sum(w)\n",
        "w"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "95tAsg-xoY25"
      },
      "source": [
        "As before, we create a random starting point and then add Gaussian noise to create offspring."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2cl4aOs5oY26"
      },
      "outputs": [],
      "source": [
        "np.random.seed(123)\n",
        "x = np.random.rand(2) * 4 - 2\n",
        "N = np.random.normal(size=(10,2))\n",
        "x_t = x + N\n",
        "fits = himmelblau(x_t[:, 0], x_t[:, 1])\n",
        "fits"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3OkjBKAAoY2_"
      },
      "source": [
        "Now, however, instead of directly using fitness values, we'll select the best individuals and update using their weighted values."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WeSO9TXfoY2_"
      },
      "outputs": [],
      "source": [
        "sorted_ids = np.argsort(fits)\n",
        "weighted = N[sorted_ids[:mu]] * w.reshape(mu, 1)\n",
        "weighted"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OOuLRAiRoY3A"
      },
      "outputs": [],
      "source": [
        "fig = plt.figure(figsize=(8, 6))\n",
        "norm = colors.Normalize(vmin=Z.min(), vmax=Z.max())\n",
        "cs = plt.contour(X, Y, Z, levels=100, cmap='Spectral', alpha=0.4, norm=norm)\n",
        "plt.quiver(x_t[:, 0], x_t[:, 1], x_t[:, 0]-x[0], x_t[:, 1]-x[1],\n",
        "           fits, scale=fits/100, scale_units='xy', cmap='Spectral', norm=norm, alpha=0.4)\n",
        "plt.quiver(x[0]+weighted[:, 0], x[1]+weighted[:, 1], weighted[:, 0], weighted[:, 1],\n",
        "           w, scale_units='xy', cmap='Spectral', norm=norm)\n",
        "plt.scatter(x[0], x[1], c='k')\n",
        "fig.colorbar(cs)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sqRsTxHSoY3A"
      },
      "source": [
        "The first point is clear: using all points in the centroid update could pull it away from nearby optima. The second point is a bit harder to understand, but motivation is this: any transformation to the search space which **preserves rank** for the top $\\mu$ individuals does not change the update. This invariance property is helpful to search because it increases the predictive power of the fitness by inducing problem equivalence classes.\n",
        "\n",
        "Hansen, N., Ros, R., Mauny, N., Schoenauer, M., & Auger, A. (2011). Impacts of invariance in search: When CMA-ES and PSO face ill-conditioned and non-separable problems. Applied Soft Computing, 11(8), 5755-5769."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m4cEoVVIoY3A"
      },
      "source": [
        "Incorporating these two changes to the previous $(\\mu, \\lambda)$ ES brings us an ES referred to as Canonical ES\n",
        "\n",
        "<img src=\"https://github.com/SupaeroDataScience/stochastic/blob/master/notebooks/imgs/canonical.png?raw=1\" width=\"40%\" height=\"auto\">\n",
        "\n",
        "Chrabaszcz, P., Loshchilov, I., & Hutter, F. (2018, July). Back to basics: benchmarking canonical evolution strategies for playing Atari. In Proceedings of the 27th International Joint Conference on Artificial Intelligence (pp. 1419-1426). [code](https://github.com/PatrykChrabaszcz/Canonical_ES_Atari)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mk3G41hdoY3B"
      },
      "source": [
        "<img src=\"https://github.com/SupaeroDataScience/stochastic/blob/master/notebooks/imgs/canonical_results.png?raw=1\" width=\"80%\" height=\"auto\">"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CQ0zRwrhoY3B"
      },
      "source": [
        "<div class=\"alert alert-warning\">\n",
        "\n",
        "<h3>Embarrassingly parallel ES</h3>\n",
        "\n",
        "All ES examples we've seen so far use sequential evaluation of individuals in a single thread. However, evaluations can be done entirely in parallel. Evaluation is by far the most computationally expensive part of the algorith and the fact that it is embarrassingly parallel means that the wall clock time of evolutionary algorithms in general can be greatly reduced by parallelizing over multiple threads.\n",
        "    \n",
        "</div>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_eIDWMjGoY3B"
      },
      "source": [
        "<div class=\"alert alert-success\">\n",
        "    <h3>Exercise 5</h3>\n",
        "    \n",
        "Consider the Rosenbrock function. How do you expect the gradient rankings to compare to the gradient information? Will they be very similar or different? Plot a sample to investigate.\n",
        "</div>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "83Z-QMg6oY3B"
      },
      "source": [
        "# <a name=\"cmaes\">5.</a> Covariance Matrix Adaptation Evolutionary Strategy"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6hL4Nv75oY3C"
      },
      "source": [
        "Covariance Matrix Adaptation Evolutionary Strategy, or CMA-ES [1, 2], is one of the most well-known evolutionary algorithms in general and is a state-of-the-art algorithm for continuous optimization. The strength of this method is that it adapts the distribution it uses to generate the next population based on the current distribution of individuals. Until now, we were limited to a Normal distribution with a static $\\sigma$. The adaptive distribution of CMA-ES means it will cross search spaces faster and narrow in more exactly on optimal points.\n",
        "\n",
        "[1] Hansen, Nikolaus, and Andreas Ostermeier. \"Adapting arbitrary normal mutation distributions in evolution strategies: The covariance matrix adaptation.\" Proceedings of IEEE international conference on evolutionary computation. IEEE, 1996.\n",
        "\n",
        "[2] Hansen, Nikolaus, and Andreas Ostermeier. \"Completely derandomized self-adaptation in evolution strategies.\" Evolutionary computation 9.2 (2001): 159-195."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yx_2dG2GoY3C"
      },
      "source": [
        "Specifically, the things that CMA-ES improves over the previous Evolutionary Strategies we've seen is that it:\n",
        "+ combines information from multiple individuals (as in $(\\mu, \\lambda)$ and Canonical ES)\n",
        "+ combines information from multiple generations\n",
        "+ transforms the distribution of the new population to match the search space\n",
        "+ adapts the step size to prevent premature convergence\n",
        "\n",
        "One improvement in CMA-ES which we've already seen is the use of **fitness rank** instead of direct fitness for the update. However, instead of directly updating a centroid, CMA-ES updates a covariance matrix."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MQcQQjcHoY3C"
      },
      "source": [
        "CMA-ES uses two principles to achieve this: maximum likelihood estimation and step-size control. We'll start with maximum likelihood estimation, which consists of increasing the probability of successful points. CMA-ES will update the population center by taking the weighted average of high-fitness individuals."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bEm4shgvoY3D"
      },
      "outputs": [],
      "source": [
        "%matplotlib inline\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib import ticker, cm\n",
        "import matplotlib.colors as colors\n",
        "import numpy as np\n",
        "import math"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jwdaJYFJoY3D"
      },
      "outputs": [],
      "source": [
        "def himmelblau(x, y):\n",
        "    return (x**2 + y - 11)**2 + (x + y**2 - 7)**2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w75lBemDoY3D"
      },
      "outputs": [],
      "source": [
        "X = np.arange(-5, 5, 0.1)\n",
        "Y = np.arange(-5, 5, 0.1)\n",
        "X, Y = np.meshgrid(X, Y)\n",
        "Z = himmelblau(X, Y)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BcONB8GcoY3E"
      },
      "outputs": [],
      "source": [
        "lam = 70\n",
        "x = np.random.rand(2) * 4 - 2\n",
        "N = np.random.normal(size=(lam,2))\n",
        "x_t = x + N\n",
        "fits = himmelblau(x_t[:, 0], x_t[:, 1])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xCEGkXH2oY3E"
      },
      "source": [
        "First, we select the top $\\mu$ individuals based on their fitness values. We'll then weight their importance by their fitness rank, using a static weight vector $w$. This weight vector $w$ is a hyperparameter in CMA-ES, but there is a standard normalized logarithmic weight scale used by most. Note that the weight is not their fitness values but rather just the rank. This has been demonstrated to aid in search in ill-formed search spaces."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TonyLr0MoY3E"
      },
      "outputs": [],
      "source": [
        "mu = 10\n",
        "sorted_ids = np.argsort(fits)\n",
        "w = np.log(mu + 1/2) - np.log(np.arange(1, mu+1))\n",
        "w /= np.sum(w)\n",
        "weighted = N[sorted_ids[:mu]] * w.reshape(mu, 1)\n",
        "s = np.mean(weighted, axis=0)\n",
        "s"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WnHeukCjoY3F"
      },
      "source": [
        "This $s$ direction vector is similar to the gradient estimation from the last exercise. However, in CMA-ES, only the top $\\mu$ individuals are used to calculate $s$, and instead of directly using fitness values, $s$ is calculated using weight based on fitness rank. We can compare the two vectors."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LwMnFwFroY3F"
      },
      "outputs": [],
      "source": [
        "A = (fits - np.mean(fits)) / np.std(fits)\n",
        "G = -np.dot(A, N) / lam"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TonBDfA7oY3F"
      },
      "outputs": [],
      "source": [
        "fig = plt.figure(figsize=(8, 6))\n",
        "norm = colors.Normalize(vmin=Z.min(), vmax=Z.max())\n",
        "cs = plt.contour(X, Y, Z, levels=100, cmap='Spectral', alpha=0.4, norm=norm)\n",
        "plt.scatter(x_t[:, 0], x_t[:, 1], c=fits, cmap='Spectral', norm=norm)\n",
        "plt.quiver(x[0], x[1], G[0], G[1], color='b', scale=4)\n",
        "plt.quiver(x[0], x[1], s[0], s[1], color='g', scale=1)\n",
        "plt.scatter(x[0], x[1], c='k')\n",
        "fig.colorbar(cs)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C2ZjzUVroY3F"
      },
      "source": [
        "While we're only using the fitness information for a ranked weighting, this direction vector $s$ can be seen as a gradient approximation. As before, we can use this gradient information to update the center of our distribution. This is the first step of maximum likelihood estimation, aligning the distribution center with the weighted average of the best solutions."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5LiMQveyoY3G"
      },
      "outputs": [],
      "source": [
        "x = x + s"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ltadlgRCoY3G"
      },
      "source": [
        "A fundamental idea behind CMA-ES is to estimate the covariance between the selected samples, and to use this covariance matrix to shape the next distribution. A covariance matrix measures the joint variability between the different dimensions of the objective function. Covariance matrices are outside of the scope of this class, but this article has more details:\n",
        "https://datascienceplus.com/understanding-the-covariance-matrix/"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o18WS4YmoY3G"
      },
      "source": [
        "We start CMA-ES with the identity matrix as the initial covariance matrix, assuming each variable is independent and has a standard deviation of 1."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e2GJMVdMoY3H"
      },
      "outputs": [],
      "source": [
        "C = np.eye(2)\n",
        "C"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3MzAoSAToY3H"
      },
      "source": [
        "At each step, we update $C = C + ss^T$. This will use the weighted sample average as an estimate of the covariance. In CMA-ES, it is referred to as the rank one covariance update. For problems of higher dimension, there is an additional matrix update."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W7yckH-1oY3H"
      },
      "outputs": [],
      "source": [
        "C = C + np.outer(s, (s).T)\n",
        "C"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m_RTKN_EoY3H"
      },
      "source": [
        "The eigen decomposition of the covariance matrix is then used to transform the Normal distribution to match the search space. This is the second part of maximum likelihood estimation: by pulling samples from a distribution fit to performant parts of the search space, there is a higher chance of sampling good individuals. The mean of the distribution $x$ and the shape given by $C$ together maximize this likelihood."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T01FW5LroY3I"
      },
      "outputs": [],
      "source": [
        "N = np.random.normal(size=(lam, 2))\n",
        "x_N = x + N\n",
        "eigenvalues, eigenvectors = np.linalg.eig(C)\n",
        "x_C = x + np.array([np.dot(eigenvectors, np.sqrt(eigenvalues) * N[i, :]) for i in range(lam)])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sDCP_DWVoY3I"
      },
      "outputs": [],
      "source": [
        "fig = plt.figure(figsize=(8, 6))\n",
        "norm = colors.Normalize(vmin=Z.min(), vmax=Z.max())\n",
        "cs = plt.contour(X, Y, Z, levels=100, cmap='Spectral', alpha=0.4, norm=norm)\n",
        "plt.scatter(x_N[:, 0], x_N[:, 1], c='k', alpha=0.3)\n",
        "plt.scatter(x_C[:, 0], x_C[:, 1], c='b')\n",
        "plt.scatter(x[0], x[1], c='r')\n",
        "fig.colorbar(cs)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rjY3H5qjoY3I"
      },
      "source": [
        "The second main feature in CMA-ES besides maximum likelihood estimation is step-size control. CMA-ES updates two different step size parameters at different time constants. The first is the update we did previously, the update to the covariance matrix. The common formula for this is\n",
        "\n",
        "$C = (1 - c_i)C + c_i s s^T$\n",
        "\n",
        "where $c_i$ is the rank one covariance matrix update learning rate. $c_i = 2/n^2$ where $n$ is the number of dimensions is a default parameter."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0msa5-EFoY3I"
      },
      "source": [
        "The other step size parameter comes in the update to the standard deviation $\\sigma$. Until now, we've assumed that $\\sigma = 1$, but in CMAES, $\\sigma$ updates at each time step."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5lf9l2-EoY3J"
      },
      "source": [
        "An effect of these two different updates, which are often referred to as two \"evolutionary paths\", is that CMA-ES will continue to search even after finding a local optima. $\\sigma$ updates faster than $C$, narrowing in on selected areas or expanding on others, while the shape of the sampled distribution is refined over time."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "84eiLEoBoY3J"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KffmUFejoY3J"
      },
      "source": [
        "<img src=\"https://github.com/SupaeroDataScience/stochastic/blob/master/notebooks/imgs/cmaes.png?raw=1\">"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7Dr7vZQaoY3J"
      },
      "source": [
        "To see CMA-ES in practice, we'll use the pycma package (https://github.com/CMA-ES/pycma). If you haven't installed it, do so here:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_vPS-tqGoY3K"
      },
      "source": [
        "`pycma` is actively maintained by the CMA-ES author and has many more algorithm tricks and features than we've discussed here. [4] provides a good review of different CMA-ES modifications.\n",
        "\n",
        "[4] Hansen, Nikolaus. \"The CMA evolution strategy: a comparing review.\" Towards a new evolutionary computation. Springer, Berlin, Heidelberg, 2006. 75-102."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QH15RNLHoY3K"
      },
      "outputs": [],
      "source": [
        "import cma\n",
        "es = cma.CMAEvolutionStrategy(2 * [0], 0.1, {'popsize': 20, 'verb_disp': 1})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pc-27PzcoY3K"
      },
      "outputs": [],
      "source": [
        "solutions = np.array(es.ask())\n",
        "es.tell(solutions, [himmelblau(x[0], x[1]) for x in solutions])\n",
        "es.disp_annotation()\n",
        "es.disp();"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qH1GGkfNoY3K"
      },
      "outputs": [],
      "source": [
        "fig = plt.figure(figsize=(8, 6))\n",
        "ax = plt.subplot()\n",
        "norm = colors.Normalize(vmin=Z.min(), vmax=Z.max())\n",
        "cs = plt.contour(X, Y, Z, levels=100, cmap='Spectral', alpha=0.4, norm=norm)\n",
        "plt.scatter(solutions[:, 0], solutions[:, 1])\n",
        "fig.colorbar(cs)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g1Z7lT0moY3M"
      },
      "outputs": [],
      "source": [
        "def generation(i, es, ax):\n",
        "    solutions = np.array(es.ask())\n",
        "    es.tell(solutions, [himmelblau(x[0], x[1]) for x in solutions])\n",
        "    \n",
        "    ax.clear()\n",
        "    ax.contour(X, Y, Z, levels=100, cmap='Spectral', alpha=0.4, norm=norm)\n",
        "    ax.scatter(solutions[:, 0], solutions[:, 1])\n",
        "    ax.set_ylim(-5, 5)\n",
        "    ax.set_xlim(-5, 5)\n",
        "    ax.plot()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "prTnzV5noY3M"
      },
      "outputs": [],
      "source": [
        "es = cma.CMAEvolutionStrategy(2 * [0], 0.1, {'popsize': 20})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vQTEHA49oY3N"
      },
      "outputs": [],
      "source": [
        "import matplotlib.animation as animation\n",
        "fig = plt.figure(figsize=(8,6))\n",
        "ax = plt.subplot()\n",
        "fig.tight_layout()\n",
        "frames = 20\n",
        "animator = animation.FuncAnimation(fig, generation, fargs=(es, ax), frames=frames, interval=250, blit=False)\n",
        "display.display(display.HTML(animator.to_html5_video()))\n",
        "plt.close();"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WBEjqvkAoY3N"
      },
      "source": [
        "<div class=\"alert alert-success\">\n",
        "    <h3>Exercise 5</h3>\n",
        "    \n",
        "CMA-ES has been widely used in many applications. Discuss one of the following application papers in your group. What is the application? How is the problem encoded in a way that CMA-ES can solve it? Is CMA-ES compared to other methods and if so, how does it do?\n",
        "    \n",
        "+ Gagn√©, C., Beaulieu, J., Parizeau, M., & Thibault, S. (2008). Human-competitive lens system design with evolution strategies. Applied Soft Computing, 8(4), 1439-1452.\n",
        "+ Bayer, P., & Finkel, M. (2007). Optimization of concentration control by evolution strategies: Formulation, application, and assessment of remedial solutions. Water resources research, 43(2).\n",
        "+ K√§mpf, J. H., & Robinson, D. (2009). A hybrid CMA-ES and HDE optimisation algorithm with application to solar energy potential. Applied Soft Computing, 9(2), 738-745.\n",
        "+ Maki, A., Sakamoto, N., Akimoto, Y., Nishikawa, H., & Umeda, N. (2020). Application of optimal control theory based on the evolution strategy (CMA-ES) to automatic berthing. Journal of Marine Science and Technology, 25(1), 221-233.\n",
        "+ Loshchilov, I., & Hutter, F. (2016). CMA-ES for hyperparameter optimization of deep neural networks. arXiv preprint arXiv:1604.07269.\n",
        "+ Fukagata, K., Kern, S., Chatelain, P., Koumoutsakos, P., & Kasagi, N. (2008). Evolutionary optimization of an anisotropic compliant surface for turbulent friction drag reduction. Journal of Turbulence, (9), N35.\n",
        "</div>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RMDdaDZtoY3N"
      },
      "source": [
        "# <a name=\"cmaes_neuro\">6.</a> CMA-ES for Neuroevolution"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lutmiZPaoY3P"
      },
      "source": [
        "We will now use CMA-ES for the Lunar Lander problem"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xhoei4gtoY3P"
      },
      "outputs": [],
      "source": [
        "np.random.seed(123)\n",
        "env = gym.make('LunarLander-v2')\n",
        "ann = NeuralNetwork(env.observation_space.shape[0], env.action_space.n)\n",
        "es = cma.CMAEvolutionStrategy(len(ann.get_params()) * [0], 0.1, {'seed': 123})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wn1sk2_DoY3P"
      },
      "outputs": [],
      "source": [
        "for i in range(20):\n",
        "    solutions = np.array(es.ask())\n",
        "    fits = [fitness(x, ann, env) for x in solutions]\n",
        "    es.tell(solutions, fits)\n",
        "    es.disp()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8RtJGAjBoY3Q"
      },
      "outputs": [],
      "source": [
        "x = es.result[0]\n",
        "-fitness(x, ann, env, visul=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eG2ASGAIoY3Q"
      },
      "source": [
        "The results on LunarLander clearly show the benefits of CMA-ES; we have found a reasonable policy in a small number of generations. Applying CMA-ES to larger neural networks remains an open challenge, however, due to the vast number of parameters in ANNs. Specifically, CMA-ES calculates the covariance of all parameters, which is $O(n^2)$."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BtrCI1KVoY3Q"
      },
      "outputs": [],
      "source": [
        "np.shape(es.sm.C)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cAecs7VfoY3Q"
      },
      "source": [
        "<div class=\"alert alert-success\">\n",
        "    <h3>Exercise 6</h3>\n",
        "    \n",
        "Compare the $(1+\\lambda)$ ES, $(\\mu,\\lambda)$ ES, and CMA-ES algorithms on Lunar Lander. Is one significantly better than the others, consistently across different initializations?\n",
        "</div>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-RhLzBdeoY3R"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "@webio": {
      "lastCommId": null,
      "lastKernelId": null
    },
    "celltoolbar": "Slideshow",
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.7"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}