{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SLOHFH94Zi8F"
      },
      "source": [
        "# Stochastic Optimization\n",
        "\n",
        "## 1.1 Random Search\n",
        "\n",
        "<a rel=\"license\" href=\"http://creativecommons.org/licenses/by-sa/4.0/\"><img alt=\"Creative Commons License\" align=\"left\" src=\"https://i.creativecommons.org/l/by-sa/4.0/80x15.png\" /></a>&nbsp;| Dennis G. Wilson | <a href=\"https://supaerodatascience.github.io/stochastic/\">https://supaerodatascience.github.io/stochastic/</a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KEAQ9zKsZi8I"
      },
      "source": [
        "The basis of all stochastic optimization algorithms is randomly sampling the search space. Our first algorithm will be simply this: randomly sampling the search space, one point at a time, always sampling according to the same distribution."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MO77uEoVZi8J"
      },
      "source": [
        "We'll start by defining a search space using a common continuous optimization benchmark: the Rosenbrock function. Its two dimensional form is here:\n",
        "\n",
        "$f(x, y)=(a-x)^2+B(y-x^2)^2$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tK6q1I85Zi8J"
      },
      "outputs": [],
      "source": [
        "def rosenbrock(x, y, a=1, B=100):\n",
        "      return (a-x)**2 + B*((y-x**2))**2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-n-5-jhxZi8K"
      },
      "source": [
        "This function is also available in its general form, which scales to $N$ dimensions, in `scipy`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5cqnoX3FZi8L"
      },
      "outputs": [],
      "source": [
        "from scipy.optimize import rosen\n",
        "\n",
        "print(rosenbrock(0.1, 0.2))\n",
        "print(rosen([0.1, 0.2]))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_xSOiw0FZi8L"
      },
      "source": [
        "To understand this search space, we'll plot it."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2fJlYEpbZi8L"
      },
      "outputs": [],
      "source": [
        "%matplotlib inline\n",
        "%matplotlib notebook\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from matplotlib import cm\n",
        "from matplotlib.ticker import LinearLocator, FormatStrFormatter\n",
        "from mpl_toolkits.mplot3d import Axes3D\n",
        "import math"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nGHdwpPtZi8L"
      },
      "outputs": [],
      "source": [
        "def plot_2d(X, Y, Z):\n",
        "    fig = plt.figure(figsize=(8, 6))\n",
        "    cs = plt.contourf(X, Y, Z, cmap=cm.nipy_spectral)\n",
        "    fig.colorbar(cs)\n",
        "    plt.show()\n",
        "    \n",
        "def plot_3d(X, Y, Z):\n",
        "    fig = plt.figure(figsize=(6, 4))\n",
        "    ax = fig.add_subplot(projection='3d')\n",
        "    surf = ax.plot_surface(X, Y, Z, cmap=cm.nipy_spectral,\n",
        "                       linewidth=0, antialiased=False)\n",
        "    fig.colorbar(surf)\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BOxzVmURZi8M"
      },
      "outputs": [],
      "source": [
        "X = np.arange(-2, 2, 0.01)\n",
        "Y = np.arange(-2, 2, 0.01)\n",
        "X, Y = np.meshgrid(X, Y)\n",
        "Z = rosenbrock(X, Y)\n",
        "plot_3d(X, Y, Z)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kbj0EwtxZi8M"
      },
      "source": [
        "As we can see, there is a minimum ridge area. Even using an uninformed search, this ridge should be easy to find. However, the global minimum is found at $(a, a^2)$, where $f(a, a^2) = 0$, and this exact point is very hard to find. In our case, this global minimum is at $(1, 1)$. Let's try to find it."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z4GQ3tZAZi8M"
      },
      "source": [
        "We'll first look at pure random search which uses a uniform distribution to sample the search space. Formally, we'll assume we're minimizing the function $f$ in\n",
        "\n",
        "$f:[A, B]^n \\rightarrow \\mathbb{R}$\n",
        "\n",
        "For the Rosenbrock function, this is not really true: the function is defined for all real number $\\mathbb{R}$, so there are no minimum and maximum values for our uniform distribution, $A, B$ (besides infinity). We'll assume we're only optimizing between -2 and 2 for both dimensions, which is clearly cheating, but that's okay."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ku9LeXzjZi8M"
      },
      "source": [
        "## Pure Random Search\n",
        "\n",
        "    Initialize uniformly at random x ∈ Ω=[A, B]\n",
        "    while not terminate\n",
        "        Sample x′ uniformly at random in Ω\n",
        "        if f(x′) < f(x)\n",
        "            x = x'\n",
        "    return x"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z1mix2QaZi8N"
      },
      "source": [
        "References about pure random search (or uniform random search), for historical purposes:\n",
        "+ S.H.  Brooks:  “Discussion  of  random  methods  for  locating  surface  maxima”.   Operations  Research  6  (1958),  pp.244–251.\n",
        "+ L.A. Rastrigin: “The convergence of the random search method in the extremal control of a many-parameter system”.Automation and Remote Control 24 (1963), pp.  1337–1342."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lbJl5OhIZi8N"
      },
      "source": [
        "<div class=\"alert alert-success\">\n",
        "    <h3>Exercise 1</h3>\n",
        "    \n",
        "Write a pure random search algorithm and apply it to the rosenbrock function. Let it run for 1000 evaluations (ie, 1000 calls to the `rosenbrock` function). What value does it converge to? Plot the convergence, keeping track of the best value f(x) over time. If you run it again, do the results change? If so, why?\n",
        "    \n",
        "Hint: for a uniform random number between A and B in python, you can use `np.random.rand()*(A-B)+B`\n",
        "</div>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WmmWjZFPZi8N"
      },
      "outputs": [],
      "source": [
        "# %load solutions/1_prs.py"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_RFOw9EvZi8N"
      },
      "source": [
        "<div class=\"alert alert-info\">\n",
        "    <h3> Bonus exercise</h3>\n",
        "    \n",
        "While the Rosenbrock function has an exact minimum which is difficult to find, we see that uniform random search quickly descends to the valley where $f(x, y) \\approx 0$. There are other problems which pure random search will have a much harder time with. A list of common functions can be found [here](https://coco.gforge.inria.fr/) and [here](https://en.wikipedia.org/wiki/Test_functions_for_optimization). As a bonus exercise, plot the Himmelblau function below and apply random search to it. Do you find the global optimum or a local optimum? Why?\n",
        "</div>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Lebdj-siZi8N"
      },
      "outputs": [],
      "source": [
        "def himmelblau(x, y):\n",
        "    return (x**2 + y - 11)**2 + (x + y**2 - 7)**2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S1gt7IzPZi8N"
      },
      "source": [
        "Searching with a uniform distibution for each sample is completely uninformed: the best values the algorithm has found so far don't inform the next sampling. Clearly, by using these previous samples, we can inform where our algorithm should search next. That is the idea behind random optimization."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ApNGIFAlZi8O"
      },
      "source": [
        "## Random Optimization\n",
        "\n",
        "Let $f: \\mathbb{R}^n → \\mathbb{R}$ be the fitness or cost function which must be minimized. Let $x ∈ \\mathbb{R}^n$ designate a position or candidate solution in the search-space.\n",
        "\n",
        "    Initialize x randomly in ℝ\n",
        "    while not terminate\n",
        "        x' = x + N(0, 1)\n",
        "        if f(x′) < f(x)\n",
        "            x = x'\n",
        "    return x\n",
        "\n",
        "where $N(0,1)$ is a normal distribution centered at 0 with a standard deviation of 1."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xsv5tAziZi8O"
      },
      "source": [
        "<div class=\"alert alert-success\">\n",
        "    <h3>Exercise 2</h3>\n",
        "    \n",
        "Modify your pure random search algorithm to implement random optimization, using a Normal distribution now to find the next point. Apply it to the rosenbrock function and again let it run for 1000 evaluations. Does this improve over pure random search? Plot the comparison of the two. For bonus points, plot the comparison over multiple runs of the search algorithms and compare their average convergence.\n",
        "</div>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ncMLUgzOZi8O"
      },
      "outputs": [],
      "source": [
        "# %load solutions/2_ro.py"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wSItnS-4Zi8O"
      },
      "source": [
        "<div class=\"alert alert-info\">\n",
        "    <h3>Discussion</h3>\n",
        "    \n",
        "The random optimization algorithm is exactly the (1+1) Evolutionary Strategy which we'll see in our next class and is also often referred to as \"hill climbing\". What are some points that could still be improved about this algorithm? Where is the search still uninformed?\n",
        "</div>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s8KpIGrzZi8O"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "@webio": {
      "lastCommId": null,
      "lastKernelId": null
    },
    "celltoolbar": "Slideshow",
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.7"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}